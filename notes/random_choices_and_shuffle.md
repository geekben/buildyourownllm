# random.choices 的使用与随机采样训练

## 1. `random.choices` 两种用法

### 两个参数：均匀随机采样

```python
# 从序列中随机选k个元素，每个元素被选中的概率相等
ix = random.choices(range(10), k=3)
# 可能结果: [2, 7, 2]  (允许重复)
```

在 `get_batch` 函数中使用：
```python
ix = random.choices(range(len(tokens) - block_size), k=batch_size)
# 从所有可能的起始位置中，随机选batch_size个位置
```

### 三个参数：加权随机采样

```python
# 根据weights权重随机采样，权重越高被选中概率越大
next_token = random.choices(
    range(vocab_size),    # 候选值范围
    weights=logits,       # 每个候选值的权重
    k=1                   # 采样数量
)[0]
```

在 `generate` 函数中使用，实现**按概率生成下一个token**：
- `logits` 是模型预测的每个token的出现次数
- 权重越高（出现次数越多）的token，被采样到的概率越大

---

## 2. 为什么随机选batch而不是顺序训练？

### ❌ 顺序训练的问题

```python
# 假设顺序训练
for i in range(0, len(tokens), block_size):
    x = tokens[i:i+block_size]
    y = tokens[i+1:i+block_size+1]
    # 训练...
```

**问题：**
1. **数据相关性**：相邻的block内容高度相似（如"春江潮水"、"春江潮水连"）
2. **梯度偏差**：连续多步看到相似数据，梯度方向趋同，模型"钻牛角尖"
3. **收敛慢**：模型在相似样本上反复优化，对整体数据分布学习不足

### ✅ 随机采样训练的好处

```python
# 随机采样
ix = random.choices(range(len(tokens) - block_size), k=batch_size)
```

**好处：**
1. **打破相关性**：每个batch的样本来自文本的不同位置，内容差异大
2. **更好的梯度估计**：每个batch的梯度更能代表整体数据分布
3. **避免局部最优**：随机性帮助模型跳出局部最优解
4. **更快收敛**：多样的训练样本让模型学到更通用的模式

---

## 3. 图解对比

```
顺序训练：
迭代1: [春江潮水连海平] ← 都是相似内容
迭代2: [海上明月共潮生]
迭代3: [滟滟随波千万里]
...模型一直在学相似的诗句结构

随机训练：
迭代1: [明月共潮生滟滟] [往事知多少] [春江潮水连] ... ← 打散
迭代2: [小楼昨夜又东风] [海上明月共] [故国不堪回首] ...
...每批数据都来自不同位置，学习更全面
```

---

## 4. 总结

| 方式 | 数据相关性 | 梯度质量 | 收敛速度 |
|------|-----------|---------|---------|
| 顺序训练 | 高（相邻样本相似） | 偏差大 | 慢 |
| 随机采样 | 低（样本打散） | 更准确 | 快 |

这是深度学习中的**标准做法**，PyTorch的 `DataLoader` 默认也会 `shuffle=True`。
