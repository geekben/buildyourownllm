# Build Your Own LLM ğŸš€

ä»é›¶å¼€å§‹ï¼Œä¸€æ­¥æ­¥æ„å»ºä¸€ä¸ªç±» GPT çš„è¯­è¨€æ¨¡å‹ã€‚ä½¿ç”¨å®‹è¯ä½œä¸ºè®­ç»ƒè¯­æ–™ï¼Œä»æœ€ç®€å•çš„ç»Ÿè®¡æ¨¡å‹é€æ­¥æ¼”è¿›åˆ°å®Œæ•´çš„ Transformer æ¶æ„ï¼Œæ¯ä¸ªç‰ˆæœ¬åªå¼•å…¥ä¸€ä¸ªæ–°æ¦‚å¿µï¼Œå¸®åŠ©ä½ çœŸæ­£ç†è§£ LLM çš„æ ¸å¿ƒåŸç†ã€‚

## å¿«é€Ÿå¼€å§‹

```bash
pip install -r requirements.txt
python babygpt_v11_hyper_params.py
```

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¼šæ ¹æ® promptï¼ˆå¦‚"æ˜¥æ±Ÿ"ã€"å¾€äº‹"ï¼‰è‡ªåŠ¨ç”Ÿæˆå®‹è¯é£æ ¼çš„æ–‡æœ¬ã€‚

## å­¦ä¹ è·¯çº¿å›¾

```
simplemodel.py                    # æœ€ç®€å•çš„ Bigram ç»Ÿè®¡æ¨¡å‹
       â†“
simplebigrammodel.py              # åŠ å…¥ Tokenizer å°è£…
       â†“
simplebigrammodel_torch.py        # PyTorch å¼ é‡é‡å†™
       â†“
pytorch_5min.py                   # PyTorch æ¢¯åº¦ä¸‹é™å…¥é—¨
       â†“
babygpt_v1.py                     # Embedding + Linearï¼ˆç¥ç»ç½‘ç»œèµ·ç‚¹ï¼‰
       â†“
babygpt_v2.py ~ babygpt_v10.py    # é€æ­¥æ·»åŠ  GPT ç»„ä»¶
       â†“
babygpt_v11_hyper_params.py       # å®Œæ•´é…ç½®ï¼ˆæ¨èè¿è¡Œï¼‰
       â†“
babygpt_v12_wandb.py              # Wandb å®éªŒè·Ÿè¸ª
       â†“
babygpt_sample_with_kvcache.py    # KV Cache æ¨ç†ä¼˜åŒ–
```

## æ¼”è¿›è·¯çº¿

é¡¹ç›®æŒ‰ç‰ˆæœ¬é€’è¿›ï¼Œæ¯ä¸€æ­¥åªå¼•å…¥**ä¸€ä¸ªæ–°æ¦‚å¿µ**ï¼š

### é˜¶æ®µä¸€ï¼šç»Ÿè®¡æ¨¡å‹

| æ–‡ä»¶ | è¯´æ˜ | å­¦ä¹ ç¬”è®° |
|------|------|----------|
| `simplemodel.py` | æœ€ç®€å•çš„ Bigram ç»Ÿè®¡æ¨¡å‹ï¼Œçº¯ Python å®ç° | |
| `simplemodel_with_comments.py` | `simplemodel.py` çš„è¯¦ç»†æ³¨é‡Šç‰ˆæœ¬ | `notes/random_choices_and_shuffle.md` |
| `simplebigrammodel.py` | åŠ å…¥ Tokenizer å°è£…ï¼Œç»“æ„æ›´æ¸…æ™° | |
| `simplebigrammodel_with_comments.py` | `simplebigrammodel.py` çš„è¯¦ç»†æ³¨é‡Šç‰ˆæœ¬ | `notes/batch_size_and_block_size.md` |
| `simplebigrammodel_torch.py` | ç”¨ PyTorch å¼ é‡é‡å†™ç»Ÿè®¡æ¨¡å‹ | `notes/pytorch_vs_python_list.md`<br>`notes/torch_clamp_multinomial.md`<br>`notes/experiment_simplebigrammodel_python_vs_torch.md` |
| `pytorch_5min.py` | PyTorch æ¢¯åº¦ä¸‹é™å…¥é—¨ | `notes/pytorch_training_mechanism.md` |

### é˜¶æ®µäºŒï¼šç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆBabyGPTï¼‰

| æ–‡ä»¶ | å¼•å…¥çš„æ–°æ¦‚å¿µ | å­¦ä¹ ç¬”è®° |
|------|-------------|----------|
| `babygpt_v1.py` | **Embedding + Linear**ï¼Œä»ç»Ÿè®¡è®¡æ•°è½¬å‘ç¥ç»ç½‘ç»œï¼Œå¼•å…¥æ¢¯åº¦ä¸‹é™è®­ç»ƒ | `notes/babygpt_v1_vs_simplebigrammodel.md`<br>`notes/estimate_loss_and_cross_entropy.md` |
| `babygpt_v2_position.py` | **Position Embedding**ï¼Œè®©æ¨¡å‹æ„ŸçŸ¥ token çš„ä½ç½®ä¿¡æ¯ | `notes/babygpt_v2_position_embedding.md` |
| `babygpt_v3_self_attention.py` | **Self-Attention**ï¼Œtoken ä¹‹é—´å¯ä»¥äº’ç›¸"äº¤æµ" | `notes/babygpt_v3_head_class_explained.md`<br>`notes/babygpt_v3_head_size_vs_n_embed.md`<br>`notes/babygpt_v3_self_attention_and_block_size.md` |
| `babygpt_v4_multihead_attention.py` | **Multi-Head Attention**ï¼Œå¤šä¸ªæ³¨æ„åŠ›å¤´å¹¶è¡Œæ•æ‰ä¸åŒæ¨¡å¼ | |
| `babygpt_v5_feedforward.py` | **Feed-Forward Network**ï¼Œå¢åŠ éçº¿æ€§å˜æ¢èƒ½åŠ› | |
| `babygpt_v6_block.py` | **Transformer Block**ï¼Œå°† Attention + FFN å°è£…ä¸ºå¯å †å çš„æ¨¡å— | |
| `babygpt_v7_residual_connection.py` | **æ®‹å·®è¿æ¥**ï¼Œç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ | |
| `babygpt_v8_projection.py` | **æŠ•å½±å±‚**ï¼ŒMulti-Head è¾“å‡ºæ˜ å°„å›åŸå§‹ç»´åº¦ | |
| `babygpt_v9_layer_norm.py` | **Layer Normalization**ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ | |
| `babygpt_v10_dropout.py` | **Dropout**ï¼Œæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ | |
| `babygpt_v11_hyper_params.py` | **è¶…å‚æ•°è°ƒä¼˜**ï¼Œ6 å±‚ 6 å¤´ 384 ç»´çš„å®Œæ•´é…ç½® | `notes/experiment_babygpt_v11_on_T4_GPU.md` |
| `babygpt_v12_wandb.py` | **Wandb é›†æˆ**ï¼Œå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ | |

### é˜¶æ®µä¸‰ï¼šæ¨ç†ä¼˜åŒ–

| æ–‡ä»¶ | è¯´æ˜ | å­¦ä¹ ç¬”è®° |
|------|------|----------|
| `babygpt_sample_with_kvcache.py` | **KV Cache** æ¨ç†ä¼˜åŒ–ï¼ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œäº¤äº’å¼ç”Ÿæˆ | |

## æœ€ç»ˆæ¨¡å‹æ¶æ„

```
Input â†’ Token Embedding + Position Embedding
      â†’ Transformer Block Ã— 6
          â”œâ”€â”€ Layer Norm â†’ Multi-Head Attention (6 heads) â†’ Residual + Dropout
          â””â”€â”€ Layer Norm â†’ Feed-Forward (384 â†’ 1536 â†’ 384) â†’ Residual + Dropout
      â†’ Layer Norm â†’ Linear â†’ Output Logits
```

## å­¦ä¹ ç¬”è®°

`notes/` ç›®å½•åŒ…å«å­¦ä¹ è¿‡ç¨‹ä¸­çš„ç¬”è®°å’Œå®éªŒè®°å½•ï¼š

**æ¦‚å¿µè§£æ**
- `batch_size_and_block_size.md` - batch_size ä¸ block_size æ¦‚å¿µè§£é‡Š
- `pytorch_training_mechanism.md` - PyTorch è®­ç»ƒæœºåˆ¶è¯¦è§£
- `estimate_loss_and_cross_entropy.md` - estimate_loss å‡½æ•°ä¸äº¤å‰ç†µæŸå¤±è¯¦è§£
- `random_choices_and_shuffle.md` - Python éšæœºé‡‡æ ·å‡½æ•°å¯¹æ¯”

**æ¨¡å‹æ¼”è¿›**
- `babygpt_v1_vs_simplebigrammodel.md` - BabyGPT v1 ä¸ Bigram æ¨¡å‹å¯¹æ¯”åˆ†æ
- `babygpt_v2_position_embedding.md` - v2 ä½ç½®ç¼–ç åŸç†è¯¦è§£
- `babygpt_v3_head_class_explained.md` - v3 Head ç±»è¯¦è§£ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å®ç°
- `babygpt_v3_head_size_vs_n_embed.md` - head_size ä¸ n_embed çš„å…³ç³»
- `babygpt_v3_self_attention_and_block_size.md` - è‡ªæ³¨æ„åŠ›ä¸ block_sizeï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰

**å®éªŒè®°å½•**
- `experiment_simplebigrammodel_python_vs_torch.md` - Python vs PyTorch å®ç°å¯¹æ¯”å®éªŒ
- `experiment_babygpt_v11_on_T4_GPU.md` - T4 GPU è®­ç»ƒå®éªŒè®°å½•

**å·¥å…·ç”¨æ³•**
- `torch_clamp_multinomial.md` - torch.clamp å’Œ torch.multinomial ç”¨æ³•
- `pytorch_vs_python_list.md` - PyTorch å¼ é‡ vs Python åˆ—è¡¨çš„ä¼˜åŒ–ç»†èŠ‚

## è®­ç»ƒè¯­æ–™

- `ci.txt`ï¼šæå–è‡ª [chinese-poetry](https://github.com/chinese-poetry/chinese-poetry) é¡¹ç›®ä¸­çš„å®‹è¯å’Œå—å”è¯ï¼Œç»è¿‡æ ¼å¼åŒ–å¤„ç†
- ä½¿ç”¨å­—ç¬¦çº§ Tokenizerï¼ˆæ¯ä¸ªæ±‰å­—/æ ‡ç‚¹ä¸ºä¸€ä¸ª tokenï¼‰

## ä¾èµ–

- Python 3.8+
- PyTorch
- wandbï¼ˆå¯é€‰ï¼Œä»… v12 ä½¿ç”¨ï¼‰

## å‚è€ƒèµ„æ–™

- [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) - Andrej Karpathy
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Jay Alammar
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - åŸå§‹ Transformer è®ºæ–‡
